import os
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import ArxivLoader
import fitz
import requests
import json
import nltk
import time
import leidenalg
import igraph as ig
from dotenv import load_dotenv
from sklearn.metrics.pairwise import cosine_similarity
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import matplotlib.pyplot as plt

nltk.download("punkt")

# åŠ è½½.envæ–‡ä»¶
load_dotenv(".env")

SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
SILICONFLOW_API_BASE = os.getenv("SILICONFLOW_API_BASE")

# ä½¿ç”¨ArxivExplorerè·å–ç›¸å…³æ–‡çŒ®æ‘˜è¦
def fetch_arxiv_summaries_byexplorer(user_question):
    url = f"https://arxivxplorer.com/?query={user_question}"

    chrome_options = Options()
    chrome_options.add_argument("--headless")  # å¯ç”¨æ— å¤´æ¨¡å¼

    # åˆå§‹åŒ–WebDriver
    driver = None
    try:
        driver = webdriver.Chrome(options=chrome_options)

        # è®¿é—®ç›®æ ‡ç½‘é¡µ
        driver.get(url)

        # ç­‰å¾…é¡µé¢åŠ è½½
        WebDriverWait(driver, 60).until(
            EC.presence_of_element_located(
                (By.CSS_SELECTOR, "h2.chakra-heading.css-1cy2trb")
            )
        )

        # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        # æ‰¾åˆ°æ‰€æœ‰åŒ…å«æ ‡é¢˜å’Œæ‘˜è¦çš„å…ƒç´ 
        titles = soup.find_all("h2", class_="chakra-heading css-1cy2trb")[
            :12
        ]  # æ ¹æ®æ‚¨æä¾›çš„ç±»åæ¥æŸ¥æ‰¾æ ‡é¢˜
        abstracts = soup.find_all("p", class_="chakra-text css-1s87rte")[
            :12
        ]  # æ ¹æ®æ‚¨æä¾›çš„ç±»åæ¥æŸ¥æ‰¾æ‘˜è¦

        titles_ = []
        abstracts_ = []
        # æ‰“å°æ ‡é¢˜å’Œæ‘˜è¦
        for title, abstract in zip(titles, abstracts):
            titles_.append(title.get_text(strip=True))
            abstracts_.append(abstract.get_text(strip=True))

        print(f"Found {len(titles_)} titles and {len(abstracts_)} abstracts.")
        return abstracts_, titles_
    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        if driver is not None:
            driver.quit()


def get_embeddings(texts, model="Pro/BAAI/bge-m3", token="your-token-here"):
    url = "https://api.siliconflow.cn/v1/embeddings"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    embeddings = []
    for text in texts:
        try:
            payload = {"model": model, "input": text, "encoding_format": "float"}
            response = requests.post(url, json=payload, headers=headers)
            response_json = json.loads(response.text)

            data = response_json["data"][0]
            embedding = data["embedding"]
            embeddings.append(embedding)
        except Exception as e:
            print(f"Error occurred while processing text: {text}")
            print(e)

    return np.array(embeddings)


# èšç±»æ–‡çŒ®æ‘˜è¦
def cluster_summaries(summaries, num_clusters=5):
    # è·å–æ‘˜è¦çš„åµŒå…¥å‘é‡
    embeddings = get_embeddings(summaries, token=SILICONFLOW_API_KEY)
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    kmeans.fit(embeddings)
    return kmeans.labels_, kmeans.cluster_centers_, embeddings


# ä½¿ç”¨Leidenç®—æ³•èšç±»æ–‡çŒ®æ‘˜è¦
def cluster_summaries_leiden(summaries, resolution=1.0, threshold=0.6):
    # è·å–æ‘˜è¦çš„åµŒå…¥å‘é‡
    embeddings = get_embeddings(summaries, token=SILICONFLOW_API_KEY)
    print("åµŒå…¥å‘é‡çš„å½¢çŠ¶ï¼š", embeddings.shape)
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = cosine_similarity(embeddings)

    print("ç›¸ä¼¼åº¦çŸ©é˜µçš„å½¢çŠ¶ï¼š", similarity_matrix.shape)
    # æ„å»ºå›¾ï¼Œå°†ç›¸ä¼¼åº¦çŸ©é˜µè½¬æ¢ä¸ºé‚»æ¥çŸ©é˜µ(å¯ä»¥è®¾ç½®é˜ˆå€¼)
    adjacency_matrix = (similarity_matrix > threshold).astype(int)

    # åˆ›å»ºå›¾å¯¹è±¡
    G = ig.Graph.Adjacency(adjacency_matrix.tolist(), mode="undirected")

    # å°†ç›¸ä¼¼åº¦ä½œä¸ºè¾¹çš„æƒé‡
    weights = []
    for i in range(len(G.es)):
        source, target = G.es[i].tuple
        weights.append(similarity_matrix[source][target])
    G.es["weight"] = weights

    # è¿è¡ŒLeidenç®—æ³•
    partition = leidenalg.find_partition(
        G,
        leidenalg.RBConfigurationVertexPartition,
        weights="weight",
        resolution_parameter=resolution,
    )

    # è·å–èšç±»æ ‡ç­¾
    labels = partition.membership

    # è®¡ç®—èšç±»ä¸­å¿ƒ(ä½¿ç”¨æ¯ä¸ªèšç±»ä¸­åµŒå…¥å‘é‡çš„å¹³å‡å€¼)
    unique_labels = np.unique(labels)
    cluster_centers = []
    for label in unique_labels:
        mask = np.array(labels) == label
        center = np.mean(embeddings[mask], axis=0)
        cluster_centers.append(center)
    cluster_centers = np.array(cluster_centers)

    # ç¡®å®šèšç±»çš„æ•°é‡
    num_communities = max(labels) + 1

    # ä¾‹å¦‚ï¼Œä½¿ç”¨'nipy_spectral' colormap
    cmap = plt.get_cmap("nipy_spectral", num_communities)

    # åˆ›å»ºé¢œè‰²åˆ—è¡¨
    colors = [cmap(i) for i in range(num_communities)]

    vertex_colors = [colors[label] for label in labels]

    # è®¾ç½®èŠ‚ç‚¹æ ‡ç­¾
    G.vs["label"] = labels

    # è®¾ç½®èŠ‚ç‚¹å¤§å°
    G.vs["size"] = 20

    # è®¾ç½®è¾¹å®½åº¦
    G.es["width"] = [1 + 2 * weight for weight in G.es["weight"]]

    # è®¡ç®—åº¦ä¸­å¿ƒæ€§
    degree_centrality = G.degree()
    # æ‰¾åˆ°æ¯ä¸ªèšç±»çš„ä»£è¡¨æ€§æ–‡çŒ®ï¼ˆåº¦ä¸­å¿ƒæ€§æœ€é«˜çš„èŠ‚ç‚¹ï¼‰
    representative_documents = {}
    for label in set(labels):
        cluster_nodes = [node for node, lab in enumerate(labels) if lab == label]
        max_degree = max(degree_centrality[node] for node in cluster_nodes)
        representative_node = [
            node for node in cluster_nodes if degree_centrality[node] == max_degree
        ]
        representative_documents[label] = representative_node[
            0
        ]  # é€‰æ‹©ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºä»£è¡¨

    # çªå‡ºæ˜¾ç¤ºä¸­å¿ƒèŠ‚ç‚¹
    for node in representative_documents.values():
        # vertex_colors[node] = "red"  # å°†ä¸­å¿ƒèŠ‚ç‚¹é¢œè‰²è®¾ç½®ä¸ºçº¢è‰²
        G.vs[node]["size"] = 50  # è®¾ç½®ä¸­å¿ƒèŠ‚ç‚¹å¤§å°

    # ç»˜åˆ¶å›¾å½¢
    visual_style = {
        "vertex_size": G.vs["size"],
        "vertex_color": vertex_colors,
        "vertex_label": G.vs["label"],
        "vertex_label_size": 20,
        "edge_width": G.es["width"],
        "layout": G.layout("fr"),  # ä½¿ç”¨Fruchterman-Reingoldå¸ƒå±€
        "bbox": (800, 800),  # å›¾å½¢å¤§å°
        "margin": 20,  # è¾¹è·
    }

    # ä¿å­˜å›¾å½¢åˆ°æ–‡ä»¶
    ig.plot(G, **visual_style, target="image.png")

    return labels, cluster_centers, embeddings, representative_documents


# æ‰“å°èšç±»ä¿¡æ¯
def print_cluster_info(cluster_labels, summaries, titles):
    # å°†æ‘˜è¦æŒ‰èšç±»åˆ†ç»„
    clustered_summaries = {}
    cluster_titles = {}
    for i, label in enumerate(cluster_labels):
        clustered_summaries.setdefault(label, []).append(summaries[i])
        cluster_titles.setdefault(label, []).append(titles[i])

    # ä¸ºæ¯ä¸ªèšç±»ç”Ÿæˆæ‘˜è¦
    community_summaries = {}
    for label, summaries in clustered_summaries.items():
        summary = generate_community_summaries(summaries)
        community_summaries[label] = summary
    community_info = ""
    # æ‰“å°æ¯ä¸ªèšç±»çš„æ‘˜è¦
    for label, summary in community_summaries.items():
        community_info += f"Community {label}: {summary}\n"
        for title in cluster_titles[label]:
            community_info += f"\n-Title: {title}\n"
        community_info += "\n"
    return community_info


# æ ¼å¼åŒ–æ‰“å°èšç±»ä¿¡æ¯
# def print_cluster_info_with_format(cluster_labels, summaries, titles):
#     # å°†æ‘˜è¦æŒ‰èšç±»åˆ†ç»„
#     clustered_summaries = {}
#     cluster_titles = {}
#     for i, label in enumerate(cluster_labels):
#         clustered_summaries.setdefault(label, []).append(summaries[i])
#         cluster_titles.setdefault(label, []).append(titles[i])

#     # ä¸ºæ¯ä¸ªèšç±»ç”Ÿæˆæ‘˜è¦
#     community_summaries = {}
#     for label, summaries in clustered_summaries.items():
#         summary = generate_community_summaries(summaries)
#         community_summaries[label] = summary

#     # æŒ‰ç…§ community label æ’åº
#     sorted_labels = sorted(community_summaries.keys())

#     community_info = ""
#     # æ‰“å°æ¯ä¸ªèšç±»çš„æ‘˜è¦
#     for label in sorted_labels:
#         summary = community_summaries[label]
#         community_info += f"<h4>Community {label}</h4>"
#         community_info += f"<p>{summary}</p>"
#         for title in cluster_titles[label]:
#             community_info += f"<li>{title}</li>"
#         community_info += "<br>"
#     return community_info


def print_cluster_info_with_format(cluster_labels, summaries, titles):
    # å°†æ‘˜è¦æŒ‰èšç±»åˆ†ç»„
    clustered_summaries = {}
    cluster_titles = {}
    for i, label in enumerate(cluster_labels):
        clustered_summaries.setdefault(label, []).append(summaries[i])
        cluster_titles.setdefault(label, []).append(titles[i])

    # ä¸ºæ¯ä¸ªèšç±»ç”Ÿæˆæ‘˜è¦
    community_summaries = {}
    for label, summaries in clustered_summaries.items():
        summary = generate_community_summaries(summaries)
        community_summaries[label] = summary

    # æŒ‰labelæ’åºå¹¶ç”Ÿæˆç¾åŒ–çš„è¾“å‡º
    community_info = ""
    sorted_labels = sorted(community_summaries.keys())

    for label in sorted_labels:
        # ä½¿ç”¨HTMLæ ¼å¼ç¾åŒ–è¾“å‡º
        community_info += f"""
        <div class="community-card">
            <h4 style="color: #1E88E5; margin-bottom: 10px;">ğŸ“‘ ç¤¾åŒº {label}</h4>
            <div class="community-summary">
                <p style="font-weight: 500; margin-bottom: 15px;">{community_summaries[label]}</p>
            </div>
            <div class="community-titles">
                <h5 style="color: #424242; margin-bottom: 8px;">ğŸ“š ç›¸å…³è®ºæ–‡ï¼š</h5>
                <ul style="list-style-type: none; padding-left: 0;">
        """

        for idx, title in enumerate(cluster_titles[label], 1):
            community_info += f'<li style="margin-bottom: 5px; padding-left: 20px; position: relative;">'
            community_info += (
                f'<span style="position: absolute; left: 0;">{idx}.</span> {title}</li>'
            )

        community_info += """
                </ul>
            </div>
        </div>
        """

    return community_info


# ä»å‘½ä»¤è¡Œè·å–ç”¨æˆ·ç¡®è®¤çš„ç±»åˆ«
def get_user_confirmed_cluster():
    confirmed_cluster = int(input("è¯·è¾“å…¥æ‚¨æ„Ÿå…´è¶£çš„èšç±»ç¼–å·ï¼ˆ0å¼€å§‹ï¼‰: "))
    return confirmed_cluster


def remove_references(text):
    reference_keywords = [
        "REFERENCES",
        "REFERENCE",
        "References",
        "Reference",
        "ACKNOWLEDGMENTS",
        "ACKNOWLEDGMENT",
        "Acknowledgments",
        "Acknowledgement",
    ]
    lowest_index = len(text)

    for keyword in reference_keywords:
        index = text.find(keyword)
        if index != -1 and index < lowest_index:
            lowest_index = index

    if lowest_index < len(text):
        print(
            f"Found ack and reference section starting with '{text[lowest_index:lowest_index+20]}...'"
        )
        return text[:lowest_index].strip()
    else:
        print("No reference section found")
        return text


def split_into_chunks(text, chunk_size=1000):
    sentences = nltk.sent_tokenize(text)
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= chunk_size:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.replace("\n", " ").strip())
            current_chunk = sentence + " "

    if current_chunk:
        chunks.append(current_chunk.replace("\n", " ").strip())

    return chunks


# å°†æœ€ç›¸å…³çš„æ–‡çŒ®åˆ‡åˆ†ä¸ºchunkså¹¶æ‰¾åˆ°æœ€ç›¸ä¼¼çš„chunk
def get_most_relevant_chunk(user_question, chunks, n=3):
    user_question_embedding = get_embeddings(
        [user_question], token=SILICONFLOW_API_KEY
    )[0]
    chunk_embeddings = get_embeddings(chunks, token=SILICONFLOW_API_KEY)

    user_question_embedding = user_question_embedding.reshape(-1, 1)

    print("user_question_embedding shape:", user_question_embedding.shape)
    print("chunk_embeddings shape:", chunk_embeddings.shape)
    dot_products = np.dot(chunk_embeddings, user_question_embedding)
    norm_a = np.linalg.norm(chunk_embeddings, axis=1, keepdims=True)
    norm_b = np.linalg.norm(user_question_embedding, axis=0, keepdims=True)
    cosine_similarities = dot_products / (norm_a * norm_b)
    cosine_similarities_flat = cosine_similarities.flatten()

    # æ‰¾åˆ°ç›¸ä¼¼åº¦æœ€é«˜çš„nä¸ªç´¢å¼•
    top_n_indices = np.argsort(cosine_similarities_flat)[-n:][::-1]

    # è·å–ç›¸åº”çš„chunks
    top_n_chunks = [chunks[i] for i in top_n_indices]

    return top_n_chunks


# æ ¹æ®ç›¸å…³çš„chunkå’Œuser_questionå›å¤
def answer_question(user_question, chunks):
    chat_model = ChatOpenAI(
        model="Qwen/Qwen2.5-72B-Instruct",
        openai_api_base=SILICONFLOW_API_BASE,
        openai_api_key=SILICONFLOW_API_KEY,
    )

    messages = [
        ("system", "è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š"),
        ("human", f"ä»æ–‡çŒ®ä¸­æ‰¾åˆ°çš„ç›¸å…³å†…å®¹ï¼š{chunks}\né—®é¢˜ï¼š{user_question}"),
    ]

    ai_msg = chat_model.invoke(messages)
    return ai_msg.content


def generate_community_summaries(summaries):
    chat_model = ChatOpenAI(
        model="Qwen/Qwen2.5-72B-Instruct",
        openai_api_base=SILICONFLOW_API_BASE,
        openai_api_key=SILICONFLOW_API_KEY,
    )

    messages = [
        ("system", "è¯·æ ¹æ®ä»¥ä¸‹æ–‡çŒ®æ‘˜è¦æ€»ç»“å‡ºè¯¥ç¤¾åŒºçš„ä¸»é¢˜ï¼Œä¸è¶…è¿‡ä¸¤å¥è¯ï¼š"),
        ("human", f"æ–‡çŒ®æ‘˜è¦ï¼š{summaries}\n ç¤¾åŒºä¸»é¢˜ï¼š"),
    ]

    ai_msg = chat_model.invoke(messages)
    return ai_msg.content


# ä¸»æµç¨‹
if __name__ == "__main__":
    start_time_1 = time.time()
    user_question = "Bi2Se3çš„èƒ½å¸¦ç»“æ„"

    summaries, titles = fetch_arxiv_summaries_byexplorer(user_question)

    cluster_labels, _, embeddings, representative_docs = cluster_summaries_leiden(
        summaries, resolution=1.0
    )

    # æ‰“å°èšç±»ä¿¡æ¯
    print_cluster_info(cluster_labels, summaries, titles)

    end_time_1 = time.time()
    elaspsed_time_1 = end_time_1 - start_time_1
    print("ç¬¬ä¸€é˜¶æ®µè€—æ—¶/sï¼š", elaspsed_time_1)

    # è·å–ç”¨æˆ·ç¡®è®¤çš„èšç±»
    confirmed_cluster = get_user_confirmed_cluster()

    start_time_2 = time.time()

    doc_index = representative_docs[confirmed_cluster]
    most_relevant_title = titles[doc_index]
    print("æœ€ç›¸å…³çš„æ–‡çŒ®ï¼š", most_relevant_title)

    pdf_text = (
        ArxivLoader(query=most_relevant_title, load_max_docs=1).load()[0].page_content
    )

    # TODO-å»é™¤å‚è€ƒæ–‡çŒ®å’Œè‡´è°¢éœ€æ”¹è¿›
    pdf_text_without_references = remove_references(pdf_text)
    print("\nLength of original text:", len(pdf_text))
    print("Length of text without references:", len(pdf_text_without_references))

    # è¾“å‡ºæå–çš„æ–‡æœ¬
    with open("extracted_text.txt", "w", encoding="utf-8") as f:
        f.write(pdf_text_without_references)

    # å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºchunks
    chunks = split_into_chunks(pdf_text_without_references, chunk_size=2000)
    top_n_chunks = get_most_relevant_chunk(user_question, chunks, n=5)

    print("æœ€ç›¸å…³çš„æ–‡çŒ®ç‰‡æ®µï¼š", top_n_chunks)
    response = answer_question(user_question, top_n_chunks)

    print("Responseï¼š", response)

    end_time_2 = time.time()
    elaspsed_time_2 = end_time_2 - start_time_2
    print("ç¬¬äºŒé˜¶æ®µè€—æ—¶/sï¼š", elaspsed_time_2)

    print("æ€»è€—æ—¶/sï¼š", elaspsed_time_1 + elaspsed_time_2)
