import streamlit as st
import os
import time
from dotenv import load_dotenv
from langchain_community.graphs import Neo4jGraph
from arxivTool.arxivTool import (
    fetch_arxiv_summaries_byexplorer,
    cluster_summaries_leiden,
    print_cluster_info_with_format,
    remove_references,
    split_into_chunks,
    get_most_relevant_chunk,
    answer_question,
)
from langchain_community.document_loaders import ArxivLoader
from neo4j_cypher_memory.chain import chain, history_graph


# åŠ è½½CSS
def load_css(css_file):
    with open(css_file, encoding="utf-8") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)


def process_text(text):
    # å¤„ç†æ‰€æœ‰å¯èƒ½çš„ç‰¹æ®Šå­—ç¬¦å’Œæ ¼å¼
    replacements = {
        "#": "",  # ç§»é™¤äº•å·
        "\n": " ",  # æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
        "\r": "",  # ç§»é™¤å›è½¦ç¬¦
        "\t": " ",  # åˆ¶è¡¨ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
        "  ": " ",  # å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    }

    for old, new in replacements.items():
        text = text.replace(old, new)

    # æ¸…ç†å¯èƒ½æ®‹ç•™çš„å¤šä½™ç©ºæ ¼
    return " ".join(text.split())


def get_latest_session_id():
    # ä½¿ç”¨CypheræŸ¥è¯¢è·å–æœ€å¤§çš„session id
    query = """
    MATCH (s:Session)
    RETURN COALESCE(MAX(s.id), 0) as max_id
    """
    result = history_graph.query(query)
    return result[0]["max_id"]


def create_new_session():
    # è·å–æœ€æ–°session idå¹¶é€’å¢
    latest_id = get_latest_session_id()
    new_session_id = int(latest_id) + 1

    return new_session_id


# è·å–å½“å‰ç”¨æˆ·æ‰€æœ‰ä¼šè¯å†å²
def get_session_history(user_id):
    query = """
    MATCH (u:User {id:$user_id})-[:HAS_SESSION]->(s:Session)
    MATCH (s)-[:LAST_MESSAGE]->(last_message)
    MATCH (last_message)-[:HAS_ANSWER]->(answer)
    RETURN s, last_message, answer
    ORDER BY last_message.date DESC
    """
    result = history_graph.query(query, params={"user_id": user_id})
    sessions = [
        {
            "session": record["s"],
            "last_message": record["last_message"],
            "last_answer": record["answer"],
        }
        for record in result
    ]
    return sessions


# è·å–æŒ‡å®šä¼šè¯çš„æ‰€æœ‰é—®é¢˜å’Œç­”æ¡ˆ
def get_session_details(user_id, session_id, window=3):
    query = (
        """
    MATCH (u:User {id:$user_id})-[:HAS_SESSION]->(s:Session {id:$session_id}),
                       (s)-[:LAST_MESSAGE]->(last_message)
    MATCH p=(last_message)<-[:NEXT*0.."""
        + str(window)
        + """]-()
    WITH p, length(p) AS length
    ORDER BY length DESC LIMIT 1
    UNWIND reverse(nodes(p)) AS node
    MATCH (node)-[:HAS_ANSWER]->(answer)
    RETURN {question:node.text, answer:answer.text} AS result
    """
    )
    result = history_graph.query(
        query, params={"user_id": user_id, "session_id": session_id}
    )
    details = [record["result"] for record in result]

    return details


# å±•ç¤ºUserå’ŒAssistantçš„èŠå¤©æ¶ˆæ¯
def display_chat_messages(messages):
    # ä½¿ç”¨ st.chat_messages æ¥æ˜¾ç¤ºæ‰€æœ‰æ¶ˆæ¯
    for msg in messages:
        if msg["role"] == "user":
            st.chat_message("user", avatar="static/user.png").write(
                msg["content"]
            )  # å‡è®¾ç”¨æˆ·æ¶ˆæ¯çš„å¤´åƒæ˜¯ user.png
        else:
            st.chat_message("assistant", avatar="static/chatbot.png").write(
                msg["content"]
            )  # å‡è®¾åŠ©æ‰‹æ¶ˆæ¯çš„å¤´åƒæ˜¯ assistant.png


# æ‰§è¡Œæ–‡çŒ®èšç±»
def perform_clustering_analysis(question):
    status_placeholder = st.empty()
    try:
        with status_placeholder.container():
            # st.info("æ­£åœ¨è¿›è¡Œæ–‡çŒ®èšç±»åˆ†æ...")
            start_time_1 = time.time()

            summaries, titles = fetch_arxiv_summaries_byexplorer(question)
            cluster_labels, _, embeddings, representative_docs = (
                cluster_summaries_leiden(summaries, resolution=1.0)
            )
            community_info = print_cluster_info_with_format(
                cluster_labels, summaries, titles
            )

            end_time_1 = time.time()
            elapsed_time_1 = end_time_1 - start_time_1

        return (
            community_info,
            elapsed_time_1,
            cluster_labels,
            titles,
            summaries,
            representative_docs,
        )

    except Exception as e:
        status_placeholder.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        return None


def display_clustering_results(question, return_results=True):
    """
    ç¬¬ä¸€é˜¶æ®µï¼šå±•ç¤ºèšç±»ç»“æœå¹¶è¿”å›ä¸­é—´ç»“æœ
    """
    result_placeholder = st.empty()

    (
        community_info,
        elapsed_time_1,
        cluster_labels,
        titles,
        summaries,
        representative_docs,
    ) = perform_clustering_analysis(question)

    if community_info is not None:
        with result_placeholder.container():
            st.markdown(
                """
                <div class="card">
                    <h3>ğŸ“Š èšç±»åˆ†æç»“æœ</h3>
                """,
                unsafe_allow_html=True,
            )

            st.write(community_info)
            st.markdown(
                f"""
                <p class="time-info">èšç±»åˆ†æè€—æ—¶ï¼š{elapsed_time_1}ç§’</p>""",
                unsafe_allow_html=True,
            )
            st.markdown("</div>", unsafe_allow_html=True)

            st.image("image.png", width=400, caption="èšç±»å¯è§†åŒ–")

            return {
                "community_info": community_info,
                "elapsed_time_1": elapsed_time_1,
                "cluster_labels": list(set(cluster_labels)),
                "titles": titles,
                "summaries": summaries,
                "representative_docs": representative_docs,
            }

    return None


def process_selected_cluster(
    clustering_results, selected_cluster, question, session_id
):
    """
    ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†é€‰ä¸­çš„èšç±»å¹¶ç”Ÿæˆç­”æ¡ˆ
    """
    start_time_2 = time.time()

    # è·å–æœ€ç›¸å…³æ–‡çŒ®
    doc_index = clustering_results["representative_docs"][selected_cluster]
    most_relevant_title = clustering_results["titles"][doc_index]
    most_relevant_summary = clustering_results["summaries"][doc_index]

    st.markdown("#### ğŸ“‘ æœ€ç›¸å…³æ–‡çŒ®")
    st.markdown(f"**æ ‡é¢˜**: {most_relevant_title}")
    st.markdown(f"**æ‘˜è¦**: {most_relevant_summary}")

    # å¤„ç†PDFæ–‡æœ¬
    pdf_text = (
        ArxivLoader(query=most_relevant_title, load_max_docs=1).load()[0].page_content
    )
    pdf_text_without_references = remove_references(pdf_text)

    chunks = split_into_chunks(pdf_text_without_references, chunk_size=2000)
    top_n_chunks = get_most_relevant_chunk(question, chunks, n=3)

    # ç”Ÿæˆæœ€ç»ˆå›ç­”
    response = answer_question(question, top_n_chunks)
    final_result = chain.invoke(
        {
            "question": question,
            "user_id": "user_123",
            "session_id": session_id,
            "arxiv_context": response,
        }
    )

    end_time_2 = time.time()
    elapsed_time_2 = end_time_2 - start_time_2
    total_time = round(clustering_results["elapsed_time_1"] + elapsed_time_2, 2)

    st.markdown("#### ğŸ’¡ æœ€ç»ˆåˆ†æç»“æœ")
    st.markdown(f"**å›ç­”**: {final_result}")

    st.markdown(
        f"""
        <p class="time-info">
            æ·±å…¥åˆ†æè€—æ—¶ï¼š{elapsed_time_2}ç§’<br>
            æ€»è€—æ—¶ï¼š{total_time}ç§’
        </p>
    """,
        unsafe_allow_html=True,
    )

    return final_result
